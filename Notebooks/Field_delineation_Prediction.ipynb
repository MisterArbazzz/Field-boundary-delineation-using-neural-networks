{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THIS SECTION SHOULD BE USED FOR PREDICTION FROM THE MODEL\n",
        "\n",
        "The input image data directory can be feeded through the \"DATA_PATH\"\n",
        "and \"NETWORK_PATH\" can be used for accessing the appropriate neural network for making the predictions"
      ],
      "metadata": {
        "id": "kAeH6-4c2MKP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GYebLccZbPcz",
        "outputId": "ba96a552-b344-4e2c-f3df-aa428d26046e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive folder\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFDIcczJKJEZ",
        "outputId": "3859ab5e-19e8-440a-ca25-512d1f362207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/karolzak/keras-unet\n",
            "  Cloning https://github.com/karolzak/keras-unet to /tmp/pip-req-build-b2_7pb_g\n",
            "  Running command git clone -q https://github.com/karolzak/keras-unet /tmp/pip-req-build-b2_7pb_g\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import typing\n",
        "import gdal\n",
        "import keras\n",
        "import tensorflow as tf\n",
        "\n",
        "from skimage import exposure\n",
        "\n",
        "# Install keras-unet library for python\n",
        "#%pip uninstall keras-unet\n",
        "%pip install git+https://github.com/karolzak/keras-unet\n",
        "\n",
        "# define dataset directory\n",
        "from google.colab import drive\n",
        "\n",
        "BASE_PATH = \"/content/drive/My Drive/field_delineation\"\n",
        "DATA_PATH = BASE_PATH + \"/data/\"\n",
        "NETWORK_PATH = BASE_PATH + \"/networks\"\n",
        "IMAGE_PATH = \"/Original\"\n",
        "LABEL_PATH = \"/Classified\"\n",
        "PREDICTION_PATH = \"/Prediction\"\n",
        "\n",
        "#INCLUDE_FOLDERS = [\"Flevoland\", \"Friesland\", \"Gelderland\", \"Limburg\", \"Overijssel\", \"Zeeland\", \"Zuid-Holland\"]\n",
        "INCLUDE_FOLDERS = [\"Norway\"]\n",
        "\n",
        "LEGEND = {\n",
        "    1: 'Other',\n",
        "    2: 'Field Boundary'\n",
        "}\n",
        "\n",
        "\n",
        "# To assess the accuracy you have to define the networks UUID and name here\n",
        "NETWORK_UUID = \"22f7adf6-f864-11ec-be5e-0242ac1c0002\"\n",
        "\n",
        "# Allowed Values:\n",
        "\n",
        "#   * FCNDK5\n",
        "#   * FCNDK6\n",
        "#   * UNet2\n",
        "#   * UNet3\n",
        "NETWORK_NAME = \"unet3\"\n",
        "\n",
        "\n",
        "# To compile the model, also the optimizer has to be defined\n",
        "NETWORK_OPTIMIZER = \"Adam\"\n",
        "\n",
        "# Make sure you use the same optimizer parameters as in the training run\n",
        "SGD_LEARNING_RATE = 0.01\n",
        "SGD_MOMENTUM = 0.9\n",
        "\n",
        "ADAM_LEARNING_RATE = 0.0015\n",
        "ADAM_BETA_1 = 0.9\n",
        "ADAM_BETA_2 = 0.999\n",
        "ADAM_EPSILON= 1e-06"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB6fZzP744dc"
      },
      "outputs": [],
      "source": [
        "# Prerequisites for the network\n",
        "\n",
        "# Includes a few helper functions which will be used to create and\n",
        "# evaluate the network, the training and the accuracy.\n",
        "import imp, h5py\n",
        "import dill, pickle\n",
        "import uuid\n",
        "imp.reload(h5py)\n",
        "\n",
        "from tensorflow.python.keras import backend as K\n",
        "sess = K.get_session()\n",
        "\n",
        "from tensorflow.compat.v1.keras.backend import set_session\n",
        "\n",
        "\n",
        "# Tensorflow configuration\n",
        "config = tf.compat.v1.ConfigProto()\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
        "set_session(tf.compat.v1.Session(config=config))\n",
        "\n",
        "\n",
        "class ModelHistory:\n",
        "    \"\"\"Just a small container class to hold relevant information of a trained\n",
        "    model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, uuid, name, model, history, readme):\n",
        "        \"\"\"Create a new instance of this class\n",
        "        :param uuid: A unique identifier of the network\n",
        "        :param name: The networks name\n",
        "        :param model: The pretrained model\n",
        "        :param history: The training history of the model\n",
        "        :param readme: A small readme with a summary of training parameters\n",
        "        \"\"\"\n",
        "        self.uuid = uuid\n",
        "        self.name = name\n",
        "        self.model = model\n",
        "        self.history = history\n",
        "        self.readme = readme\n",
        "\n",
        "\n",
        "def get_file_names(uuid, name):\n",
        "    \"\"\"Generates three file names for the model, weights and history file and\n",
        "    the networks readme.\n",
        "\n",
        "    File name order of returned tuple:\n",
        "        * readme\n",
        "        * model\n",
        "        * weights\n",
        "        * history\n",
        "\n",
        "    :param uuid: Universal unique identifier of a trained network\n",
        "    :param name: The networks name\n",
        "    :return: Tuple with files in the order mentioned above\n",
        "    \"\"\"\n",
        "    base = f\"{NETWORK_PATH}/{str(uuid)}-{name}\"\n",
        "\n",
        "    f_readme = f\"{base}-readme.txt\"\n",
        "    f_model = f\"{base}-model.h5\"\n",
        "    f_weights = f\"{base}-weights.h5\"\n",
        "    f_history = f\"{base}-history\"\n",
        "\n",
        "    return (f_readme, f_model, f_weights, f_history)\n",
        "\n",
        "\n",
        "def export_model(m: ModelHistory):\n",
        "    \"\"\"If a model is sufficiently trained, it can be exported. This allows to\n",
        "    simply save the models state and the training history. Whenever one want to\n",
        "    use the model the next time, the training can be skipped, since the trained\n",
        "    model can just be imported from files.\n",
        "\n",
        "    :param model_history: The trained model and history to be stored\n",
        "    \"\"\"\n",
        "    f_readme, f_model, f_weights, f_history = get_file_names(m.uuid,m.name)\n",
        "\n",
        "    # save readme\n",
        "    with open(f_readme, 'w') as f:\n",
        "        f.write(m.readme)\n",
        "    print(f\"Exported README: {f_readme}\")\n",
        "\n",
        "    # save models & weights\n",
        "    m.model.save(f_model)\n",
        "    print(f\"Exported model: {f_model}\")\n",
        "    m.model.save_weights(f_weights)\n",
        "    print(f\"Exported weights: {f_weights}\")\n",
        "\n",
        "    # save history\n",
        "    with open(f_history, \"wb\") as f:\n",
        "        pickle.dump(m.history, f)\n",
        "    print(f\"Exported history: {f_history}\")\n",
        "\n",
        "def import_model(uuid, name):\n",
        "    \"\"\"Previously exported models can be imported with this funciton.\n",
        "    :param uuid: The networks uuid\n",
        "    :param name: The networks name\n",
        "    :return: Instance of ModelHistory\n",
        "    \"\"\"\n",
        "    f_readme, f_model, f_weights, f_history = get_file_names(uuid, name)\n",
        "\n",
        "    # Load readme\n",
        "    with open(f_readme, 'r') as f:\n",
        "        readme = \"\".join(f.readlines())\n",
        "    print(f\"Imported README: {f_readme}\")\n",
        "\n",
        "    # Load model & weights\n",
        "    model = tf.keras.models.load_model(f_model)\n",
        "    print(f\"Imported model: {f_model}\")\n",
        "    model.load_weights(f_weights)\n",
        "    print(f\"Imported weights: {f_weights}\")\n",
        "\n",
        "    # Load history\n",
        "    with open(f_history, 'rb') as f:\n",
        "        history = pickle.load(f)\n",
        "    print(f\"Imported history: {f_history}\")\n",
        "\n",
        "    return ModelHistory(uuid, name, model, history, readme)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDAHrTgfawNZ",
        "outputId": "7b0061ba-4ae1-4e41-939a-2a22221d0c12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of image & label tiles: 1\n"
          ]
        }
      ],
      "source": [
        "# Loading input data\n",
        "#\n",
        "# Input data is loaded into two dictionaries: \n",
        "#\n",
        "# images: contains the 4-band images. The values are loaded as ints.\n",
        "# labels: contains 3D arrays in which each pixel is assigned with a label \"1\" = other and \"2\" = field boundary\n",
        "\n",
        "\n",
        "def key_generator(file_name):\n",
        "    \"\"\"Generates the key of a file based on the file name. The resulting key is\n",
        "    a tuple of the province as string & the file number index as int,\n",
        "    e.g. (\"gelderland\", 29)\n",
        "    \"\"\"\n",
        "    file_name = file_name.lower()\n",
        "    file_name = file_name.replace(\"classified_\", \"\")\n",
        "    file_name = file_name.replace(\"original_\", \"\")\n",
        "    file_name = file_name.replace(\".tif\", \"\")\n",
        "    # TODO: Some images are named incorrectly\n",
        "    #       (e.g. no '_' between the province name and the image index)\n",
        "    (province, index) = tuple(file_name.split(\"_\"))\n",
        "    index = int(index)\n",
        "    return (province, index)\n",
        "\n",
        "def gtiff_to_array(file_path):\n",
        "    \"\"\"Takes a file path and returns a tif file as a 3-dimensional numpy array, width x height x bands.\"\"\"\n",
        "    data = gdal.Open(file_path)\n",
        "    bands = [data.GetRasterBand(i+1).ReadAsArray() for i in range(data.RasterCount)]\n",
        "    return np.stack(bands, axis=2)\n",
        "\n",
        "# Dictionaries which contain the input data\n",
        "x_dict = {}\n",
        "#y_dict = {}\n",
        "\n",
        "# Iterate through defined folders and load all image data into the dictionaries\n",
        "# image_data and label_data. Images can be accessed with (<province>, <index>)\n",
        "for folder in INCLUDE_FOLDERS:\n",
        "    original = DATA_PATH + folder + IMAGE_PATH\n",
        "    #classified = DATA_PATH + folder + LABEL_PATH\n",
        "    for f in os.listdir(original): \n",
        "        key = key_generator(f)\n",
        "        x_dict[key] = gtiff_to_array(original + \"/\" + f)\n",
        "\n",
        "print(f\"Total number of image & label tiles: {len(x_dict)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9487wKIZKZpU",
        "outputId": "7bd7168f-6698-4c17-e7eb-22623eeb238a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Performed normalization of norway_1\n"
          ]
        }
      ],
      "source": [
        "# Normalizing images\n",
        " \n",
        "# Normalizing all input images into values in the interval [0, 1].\n",
        "# All bands are normalized seperately, which means, the min & max\n",
        "# of each band is calculated based on each band of the image data.\n",
        "\n",
        "def normalize(val, min, max):\n",
        "    \"\"\"Normalizes the value of a single pixel. Takes into account the minimum,\n",
        "    and maximum value.\n",
        "\n",
        "    v_normalized = (v - min) / (max - min)\n",
        "\n",
        "    :param val: integer value\n",
        "    :param min: integer minimum\n",
        "    :param max: integer maximum\n",
        "    :return: single floating point number in [0, 1]\n",
        "    \"\"\"\n",
        "    # TODO: check if necessary, otherwise delete\n",
        "    return (val - min) / (max - min)\n",
        "\n",
        "\n",
        "def normalize_array(arr):\n",
        "    \"\"\"Takes a 3D array as input, iterates over the bands and normalizes those.\n",
        "\n",
        "    :param arr: input array (original image data) \n",
        "    :return: normalized data with values between 0 and 1\n",
        "    \"\"\"\n",
        "    arr_norm = np.zeros(arr.shape, dtype=np.float32)\n",
        "\n",
        "    for i in range(arr.shape[2]):\n",
        "        min = arr[:, :, i].min()\n",
        "        max = arr[:, :, i].max()\n",
        "\n",
        "        arr_norm = (arr - min) / (max - min)\n",
        "    \n",
        "    return arr_norm\n",
        "\n",
        "\n",
        "for k, v in x_dict.items():\n",
        "    x_dict[k] = normalize_array(v)\n",
        "    print(f\"Performed normalization of {k[0]}_{k[1]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzXD96NtZ3L1",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "import model_segnet_Nbands\n",
        "\n",
        "from keras.layers import Activation, BatchNormalization, Convolution2D, LeakyReLU, Reshape, ZeroPadding2D\n",
        "from keras.models import Sequential\n",
        "\n",
        "from tensorflow.keras.optimizers import SGD, Adam\n",
        "\n",
        "from keras_unet.models import satellite_unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qba8oSlkZ3Lu",
        "outputId": "14370d9a-2d0c-4e98-91e3-74b023dbf0ea",
        "scrolled": false
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Imported README: /content/drive/My Drive/field_delineation/networks/22f7adf6-f864-11ec-be5e-0242ac1c0002-unet3-readme.txt\n",
            "Imported model: /content/drive/My Drive/field_delineation/networks/22f7adf6-f864-11ec-be5e-0242ac1c0002-unet3-model.h5\n",
            "Imported weights: /content/drive/My Drive/field_delineation/networks/22f7adf6-f864-11ec-be5e-0242ac1c0002-unet3-weights.h5\n",
            "Imported history: /content/drive/My Drive/field_delineation/networks/22f7adf6-f864-11ec-be5e-0242ac1c0002-unet3-history\n",
            "Load existing network: 22f7adf6-f864-11ec-be5e-0242ac1c0002 unet3\n",
            " \n",
            "Training Config\n",
            "-------------------------------------\n",
            "NETWORK\n",
            "  Uuid:               22f7adf6-f864-11ec-be5e-0242ac1c0002\n",
            "  Name:               unet3\n",
            "  Optimizer:          Adam\n",
            "\n",
            "\n",
            "PARAMETERS\n",
            "  Bands:              4\n",
            "  Classes:            2\n",
            "  Epochs:             800\n",
            "  Batch Size:         8\n",
            "\n",
            "OPTIMIZER (Adam)\n",
            "  Learning Rate:    0.0015\n",
            "  Beta1:            0.9\n",
            "  Beta2:            0.999\n",
            "  Epsilon:          1e-06\n",
            "  \n",
            "EXECUTION SUMMARY\n",
            "  Patches:          433\n",
            "  Validation Split: 0.05\n",
            "  Resolution:       400x400\n",
            "  Bands:            4\n",
            "  Classes:          1\n",
            "  \n",
            "Training Set:\n",
            "     (flevoland, 93)\n",
            "    (friesland, 152)\n",
            "    (zeeland, 35)\n",
            "    (friesland, 123)\n",
            "    (flevoland, 80)\n",
            "    (overijssel, 21)\n",
            "    (zeeland, 48)\n",
            "    (friesland, 178)\n",
            "    (zeeland, 7)\n",
            "    (zeeland, 20)\n",
            "    (overijssel, 60)\n",
            "    (overijssel, 84)\n",
            "    (zeeland, 62)\n",
            "    (overijssel, 71)\n",
            "    (flevoland, 64)\n",
            "    (overijssel, 5)\n",
            "    (zeeland, 64)\n",
            "    (flevoland, 51)\n",
            "    (gelderland, 37)\n",
            "    (overijssel, 33)\n",
            "    (zeeland, 36)\n",
            "    (overijssel, 44)\n",
            "    (overijssel, 31)\n",
            "    (zeeland, 8)\n",
            "    (overijssel, 59)\n",
            "    (friesland, 137)\n",
            "    (zeeland, 18)\n",
            "    (flevoland, 22)\n",
            "    (friesland, 148)\n",
            "    (overijssel, 17)\n",
            "    (friesland, 163)\n",
            "    (overijssel, 15)\n",
            "    (zeeland, 24)\n",
            "    (flevoland, 107)\n",
            "    (overijssel, 56)\n",
            "    (friesland, 138)\n",
            "    (flevoland, 94)\n",
            "    (overijssel, 43)\n",
            "    (zeeland, 34)\n",
            "    (limburg, 165)\n",
            "    (zuid-holland, 100)\n",
            "    (overijssel, 1)\n",
            "    (friesland, 179)\n",
            "    (zeeland, 6)\n",
            "    (limburg, 82)\n",
            "    (zuid-holland, 114)\n",
            "    (flevoland, 65)\n",
            "    (zeeland, 50)\n",
            "    (friesland, 180)\n",
            "    (overijssel, 6)\n",
            "    (flevoland, 52)\n",
            "    (friesland, 195)\n",
            "    (overijssel, 34)\n",
            "    (overijssel, 45)\n",
            "    (zeeland, 77)\n",
            "    (friesland, 134)\n",
            "    (friesland, 149)\n",
            "    (overijssel, 18)\n",
            "    (overijssel, 29)\n",
            "    (friesland, 175)\n",
            "    (overijssel, 74)\n",
            "    (zeeland, 10)\n",
            "    (gelderland, 29)\n",
            "    (flevoland, 108)\n",
            "    (zeeland, 31)\n",
            "    (overijssel, 57)\n",
            "    (flevoland, 95)\n",
            "    (flevoland, 20)\n",
            "    (friesland, 150)\n",
            "    (zeeland, 33)\n",
            "    (limburg, 166)\n",
            "    (zuid-holland, 101)\n",
            "    (friesland, 176)\n",
            "    (overijssel, 2)\n",
            "    (zeeland, 5)\n",
            "    (gelderland, 30)\n",
            "    (limburg, 96)\n",
            "    (zeeland, 47)\n",
            "    (flevoland, 79)\n",
            "    (zeeland, 49)\n",
            "    (flevoland, 66)\n",
            "    (overijssel, 7)\n",
            "    (zeeland, 21)\n",
            "    (overijssel, 46)\n",
            "    (zeeland, 63)\n",
            "    (overijssel, 70)\n",
            "    (zeeland, 76)\n",
            "    (overijssel, 4)\n",
            "    (flevoland, 50)\n",
            "    (flevoland, 37)\n",
            "    (friesland, 135)\n",
            "    (overijssel, 98)\n",
            "    (overijssel, 32)\n",
            "    (zeeland, 37)\n",
            "    (overijssel, 19)\n",
            "    (overijssel, 30)\n",
            "    (zeeland, 9)\n",
            "    (flevoland, 34)\n",
            "    (friesland, 136)\n",
            "    (overijssel, 58)\n",
            "    (flevoland, 21)\n",
            "    (friesland, 151)\n",
            "    (friesland, 162)\n",
            "    (overijssel, 16)\n",
            "    (overijssel, 3)\n",
            "    (friesland, 177)\n",
            "    (limburg, 110)\n",
            "    (overijssel, 14)\n",
            "    (zeeland, 4)\n",
            "Test set:\n",
            "         (zeeland, 32)\n",
            "    (friesland, 165)\n",
            "    (zeeland, 3)\n",
            "    (zeeland, 22)\n",
            "    (friesland, 164)\n",
            "    (overijssel, 35)\n",
            "    (flevoland, 36)\n",
            "    (zeeland, 23)\n",
            "    (friesland, 166)\n",
            "    (overijssel, 20)\n",
            "    (zeeland, 19)\n",
            "    (zeeland, 51)\n",
            "    (friesland, 161)\n"
          ]
        }
      ],
      "source": [
        "# Creating network\n",
        "\n",
        "# In this code block, the network configuration is loaded properly\n",
        "# and the corresponding builder function is called.\n",
        "\n",
        "NUMBER_BANDS = 4\n",
        "NUMBER_CLASSES = 2\n",
        "NUMBER_EPOCHS = 10\n",
        "NUMBER_BATCHES = 64\n",
        "VALIDATION_SPLIT = 0.02\n",
        "\n",
        "model_builder = build_network(NETWORK_NAME)\n",
        "\n",
        "# Load existing network from files\n",
        "m = import_model(NETWORK_UUID, NETWORK_NAME)\n",
        "readme = m.readme\n",
        "model = m.model\n",
        "history = m.history\n",
        "print(f\"Load existing network: {NETWORK_UUID} {NETWORK_NAME}\")\n",
        "\n",
        "# Printing configuration README before training\n",
        "print(readme)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please note that most of the helper functions are repeated in testing and prediction section to maintain the code legacy and to avoid variable mislocation"
      ],
      "metadata": {
        "id": "P6kqU7uu3Ebt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yaknOjT38cZS"
      },
      "outputs": [],
      "source": [
        "# Predictions & export\n",
        "# Here, the network is fed with the given input files. Resulting\n",
        "# predictions are exported as TIF files.\n",
        "\n",
        "from matplotlib import pyplot\n",
        "from PIL import Image\n",
        "\n",
        "def evaluate_predictions(\n",
        "    input: np.ndarray,\n",
        "    nc: int,\n",
        "    f_weights: str,\n",
        "    optimizer: tf.keras.optimizers.Optimizer,\n",
        "    model_builder: typing.Callable,\n",
        ") -> np.ndarray:\n",
        "    \"\"\"Takes an input image, patches it into smaller patches and feeds the model\n",
        "    with each of the patches. The output samples are patches of the predicted\n",
        "    classification. These patches are combined into one large image, that can\n",
        "    be compared with the classified image of the corresponding input data.\n",
        "\n",
        "    :param input: test image to evaluate\n",
        "    :param nc: Number of classes/labels\n",
        "    :param f_weights: File path to the corresponding weights file\n",
        "    :param optimizer: Optimizer for the network\n",
        "    :param model_build: method to create the model\n",
        "    :return: 2D ndarray of the predicted labels\n",
        "    \"\"\"\n",
        "    x, y, bands = input.shape\n",
        "\n",
        "    # Build model and load model weights\n",
        "    model = model_builder(x, y, bands, nc)\n",
        "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics='accuracy')\n",
        "    model.load_weights(f_weights)\n",
        "\n",
        "    # Predict field boundaries in network\n",
        "    # Increase dimension to perform batch prediction\n",
        "    input = np.expand_dims(input, 0)\n",
        "    prediction = model.predict(input)[0]\n",
        "    # Map highest score onto label\n",
        "    prediction = np.argmax(prediction, axis=2) + 1\n",
        "    \n",
        "    return prediction\n",
        "\n",
        "def export_array(labels: np.ndarray, filename: str):\n",
        "    \"\"\"Maps labels onto the colors black & white and exports the resulting image\n",
        "    as a file with the given file.\n",
        "\n",
        "    :param labels: 2D array of labels per pixel\n",
        "    :param filename: File name of the new file\n",
        "    \"\"\"\n",
        "    x, y = labels.shape\n",
        "\n",
        "    img = np.zeros((x, y, 3), dtype=np.uint8)\n",
        "\n",
        "    for i in range(img.shape[2]):\n",
        "        img[:, :, i] = np.where(labels[:, :] == 2, 255, 0)\n",
        "\n",
        "    Image.fromarray(img).save(filename)\n",
        "    #Image.fromarray(img).save()\n",
        "    #pyplot.imsave(filename, Image.fromarray(img))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqWLGyUIJjm4",
        "outputId": "e81aaab2-f846-4349-c2b7-b704ec208ca5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 451ms/step\n",
            "Exported prediction /content/drive/My Drive/field_delineation/data/Norway/Prediction_unet3/prediction_norway_1.tif\n"
          ]
        }
      ],
      "source": [
        "# Predict images and store as TIF\n",
        "\n",
        "# This code block will iterate through the included folders and\n",
        "# performs predictions on the satellite image tiles. The resulting\n",
        "# images are mapped onto black & white and then exported as TIF files\n",
        "\n",
        "import pathlib\n",
        "\n",
        "\n",
        "keys_per_province = dict()\n",
        "\n",
        "for f in INCLUDE_FOLDERS:\n",
        "    keys_per_province[f] = [k for k in x_dict.keys() if k[0].lower() == f.lower()]\n",
        "\n",
        "\n",
        "for f, keys in keys_per_province.items():\n",
        "    for k in keys:\n",
        "        x = x_dict[k]\n",
        "\n",
        "        (_, _, f_weights, _) = get_file_names(NETWORK_UUID, NETWORK_NAME)\n",
        "\n",
        "        try:\n",
        "            img = evaluate_predictions(\n",
        "                x,\n",
        "                NUMBER_CLASSES,\n",
        "                f_weights,\n",
        "                OPTIMIZER,\n",
        "                model_builder,\n",
        "            )\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to predict image of size {x.shape[0]}x{x.shape[1]}\")\n",
        "            continue\n",
        "\n",
        "        # Build directory name\n",
        "        fname = DATA_PATH\n",
        "        fname += f\"{f}/\"\n",
        "        fname += f\"Prediction_{NETWORK_NAME}/\"\n",
        "\n",
        "        # Create directory\n",
        "        pathlib.Path(fname).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # build filename\n",
        "        fname += f\"prediction_{k[0]}_{k[1]}\"\n",
        "        fname += \".tif\"\n",
        "\n",
        "        export_array(img, fname)\n",
        "        print(f\"Exported prediction {fname}\")"
      ]
    }
  ]
}